[
  {
    "id": "gb0000",
    "title": "GB0000",
    "url": "gb/gb0000.html",
    "content": "All __torch_function__ overrides returned NotImplemented due to TypeError from user code fn={fn}, args={args}, kwargs={kwargs} All __torch_function__ overrides for for function {fn} returned NotImplemented - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0001",
    "title": "GB0001",
    "url": "gb/gb0001.html",
    "content": "Argument of `as_subclass` must be a non-dispatcher-style tensor subclass {self}.as_subclass({cls}) Currently not supported - Avoid this call or move it outside `torch.compile` regione - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0002",
    "title": "GB0002",
    "url": "gb/gb0002.html",
    "content": "Assertion failed on symbolic shapes str(sym_expr)  - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0003",
    "title": "GB0003",
    "url": "gb/gb0003.html",
    "content": "Attempt to trace generator  Generators cannot be compiled directly with `torch.compile`. - Call a generator from inside of a non-generator Python function and  - compile that function instead. - This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround."
  },
  {
    "id": "gb0004",
    "title": "GB0004",
    "url": "gb/gb0004.html",
    "content": "Attempted super().__delattr__() on an object without mutation tracking call_method {self} {name} Dynamo needs to track mutations on an object before `super().__delattr__` can be used on it. But the object ({self.objvar}) doesn't have attribute mutation tracking enabled. - Ensure the object is tracked by Dynamo's side effect system. - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0005",
    "title": "GB0005",
    "url": "gb/gb0005.html",
    "content": "Attempted to a str() method implemented in C/C++  {type(arg.value)} has a C/C++ based str method. This is not supported. - Write the str method in Python"
  },
  {
    "id": "gb0006",
    "title": "GB0006",
    "url": "gb/gb0006.html",
    "content": "Attempted to call a super() attribute that is not a function or method call_method {self} {name} Dynamo does not know how to trace the call `super().{name}()` because `super().{name}` is not a function or method attribute. - Ensure the attribute accessed via `super()` is a standard method or function."
  },
  {
    "id": "gb0007",
    "title": "GB0007",
    "url": "gb/gb0007.html",
    "content": "Attempted to call function marked as skipped module: {module_name}, qualname: {qualname}, skip reason: {reason} explanation *No hints provided.*"
  },
  {
    "id": "gb0008",
    "title": "GB0008",
    "url": "gb/gb0008.html",
    "content": "Attempted to inline function marked as skipped qualname: {fn_qualname}, name: {func.get_name()}, filename: `{func.get_filename()}`, skip reason: {result.reason} Dynamo developers have intentionally marked that the function `{fn_qualname}` should not be traced. *No hints provided.*"
  },
  {
    "id": "gb0009",
    "title": "GB0009",
    "url": "gb/gb0009.html",
    "content": "Attempted to inline function marked as skipped (SkipFunctionVariable) Attempted to inline a SkipFunctionVariable {func} Attempted to inline a function that was previously determined to be marked as intentionally skipped. *No hints provided.*"
  },
  {
    "id": "gb0010",
    "title": "GB0010",
    "url": "gb/gb0010.html",
    "content": "Attempted to read a deleted variable item: {item}, name: {name}  - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0011",
    "title": "GB0011",
    "url": "gb/gb0011.html",
    "content": "Attempted to read undefined local variable LOAD_FAST {name} Could not find a local variable with name `{name}` - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0012",
    "title": "GB0012",
    "url": "gb/gb0012.html",
    "content": "Attempted to read undefined local variable (implicit) LOAD_FAST {name} Could not find an implicit local variable with name `{name}` - This happens in dict/list comprehensions - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0013",
    "title": "GB0013",
    "url": "gb/gb0013.html",
    "content": "Attempted to represent unregistered RemovableHandle  Dynamo attempted to build a representation of a torch.utils.hooks.RemovableHandle, which is not supported. This happens because the RemovableHandle was created in another frame. *No hints provided.*"
  },
  {
    "id": "gb0014",
    "title": "GB0014",
    "url": "gb/gb0014.html",
    "content": "Attempted to wrap RNN, GRU, or LSTM str(value) Dynamo does not support RNN, GRU, or LSTM. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0015",
    "title": "GB0015",
    "url": "gb/gb0015.html",
    "content": "Attempted to wrap sparse Tensor  torch.compile does not support sparse Tensors - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0016",
    "title": "GB0016",
    "url": "gb/gb0016.html",
    "content": "Attempted to wrap strided NestedTensor  torch.compile does not support strided NestedTensor *No hints provided.*"
  },
  {
    "id": "gb0017",
    "title": "GB0017",
    "url": "gb/gb0017.html",
    "content": "Attempted to wrap torch._higher_order_ops.invoke_subgraph  Directly using invoke_subgraph is not supported. Use nested_compile_region *No hints provided.*"
  },
  {
    "id": "gb0018",
    "title": "GB0018",
    "url": "gb/gb0018.html",
    "content": "Attempted to wrap unbacked SymInt  Unbacked SymInt input is not supported yet. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0019",
    "title": "GB0019",
    "url": "gb/gb0019.html",
    "content": "AutogradFunctionContextVariable escaped Dynamo-traced region  We cannot reconstruct a torch.autograd.Function's context object. *No hints provided.*"
  },
  {
    "id": "gb0020",
    "title": "GB0020",
    "url": "gb/gb0020.html",
    "content": "BUILD_STRING key conflict format_string_parts: {format_string_parts}, kwargs: {kwargs}, part.sym_kwargs: {part.sym_kwargs} Failed to build format string due to key conflict - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0021",
    "title": "GB0021",
    "url": "gb/gb0021.html",
    "content": "BUILD_STRING type error str(part) Format string part type is not correct - expected constant or format string. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0022",
    "title": "GB0022",
    "url": "gb/gb0022.html",
    "content": "Bad import result typestr(value) Import result is not a Python module. *No hints provided.*"
  },
  {
    "id": "gb0023",
    "title": "GB0023",
    "url": "gb/gb0023.html",
    "content": "Builtin `operator.*` comparison with constant `self` failed call_method {self} {name} {args} {kwargs} \"Failed to compare {self} with {other}, \"                     + f\"because {other} is not a Python constant or its mutation check fails.\" *No hints provided.*"
  },
  {
    "id": "gb0024",
    "title": "GB0024",
    "url": "gb/gb0024.html",
    "content": "CLEANUP_THROW with StopIteration  Received StopIteration when handling generator.throw/close. This is not supported. *No hints provided.*"
  },
  {
    "id": "gb0025",
    "title": "GB0025",
    "url": "gb/gb0025.html",
    "content": "Call to `torch._dynamo.graph_break()` Called `torch._dynamo.graph_break()` with args `{args}`, kwargs `{kwargs}` User-inserted graph break. Message: {graph_break_msg} - Remove the `torch._dynamo.graph_break()` call."
  },
  {
    "id": "gb0026",
    "title": "GB0026",
    "url": "gb/gb0026.html",
    "content": "Calling subclass default constructor with more than tensor argument {self.value}(args={args}, kwargs={kwargs}) Currently not supported - Avoid this constructor call or move it outside  - `torch.compile` regione - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0027",
    "title": "GB0027",
    "url": "gb/gb0027.html",
    "content": "Cannot check Tensor object identity without its fake value str(fake_tensor) TensorVariable is missing a fake example_value. - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0028",
    "title": "GB0028",
    "url": "gb/gb0028.html",
    "content": "Caught non-Exception value str(exc_instance) Except expects to receive an object of Exception type but received {exc_instance}. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0029",
    "title": "GB0029",
    "url": "gb/gb0029.html",
    "content": "Compilation of intermediate hooks requires compiled autograd var_getattr {self} {name} Dynamo must be in compiled_autograd to register hooks. *No hints provided.*"
  },
  {
    "id": "gb0030",
    "title": "GB0030",
    "url": "gb/gb0030.html",
    "content": "ComptimeContext graph break msg Manually triggered ComptimeContext graph break with message {msg}. *No hints provided.*"
  },
  {
    "id": "gb0031",
    "title": "GB0031",
    "url": "gb/gb0031.html",
    "content": "Custom __getattribute__ in nn.Module attribute access var_getattr {self} {name} Dynamo does not support checking key existence on `nn.Module` instances that have a custom `__getattribute__` method defined. - Avoid defining `__getattribute__` in your module. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0032",
    "title": "GB0032",
    "url": "gb/gb0032.html",
    "content": "Custom __getattribute__ in nn.Module dict key check has_key_in_generic_dict {self} {key} Dynamo does not support checking key existence on `nn.Module` instances that have a custom `__getattribute__` method defined. - Avoid defining `__getattribute__` in your module. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0033",
    "title": "GB0033",
    "url": "gb/gb0033.html",
    "content": "Data dependent operator str(cause.func) Operator `{cause.func}` has a non-Tensor output whose value is dependent on the data of Tensor inputs. *No hints provided.*"
  },
  {
    "id": "gb0034",
    "title": "GB0034",
    "url": "gb/gb0034.html",
    "content": "Data-dependent assertion failed (cannot compile partial graph) value: {value} Dynamo has determined when encountering a data-dependent assert failure that it should not compile the partial graph. - Use `torch._assert()` to raise a hard AssertionError when the check fails.  - This error will propagate back the user code  - that called the compiled function (i.e. Dynamo will not trace any exception handling). - Remove the assert statement. - Move the assert statement outside of any context managers in order to graph break with  - partial graph compilation (if fullgraph=False). - This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround."
  },
  {
    "id": "gb0035",
    "title": "GB0035",
    "url": "gb/gb0035.html",
    "content": "Data-dependent branching with non-constant __bool__ method: {x}, result: {result} Attempted to perform data-dependent branching on a user-defined object with a __bool__ method that did not return a constant. *No hints provided.*"
  },
  {
    "id": "gb0036",
    "title": "GB0036",
    "url": "gb/gb0036.html",
    "content": "Dynamic shape operator str(cause.func) Operator `{cause.func}`'s output shape depends on input Tensor data. - Enable tracing of dynamic shape operators with  - `torch._dynamo.config.capture_dynamic_output_shape_ops = True`"
  },
  {
    "id": "gb0037",
    "title": "GB0037",
    "url": "gb/gb0037.html",
    "content": "Dynamic shape operator (no meta kernel) str(cause.func) Operator `{cause.func}` does not have a meta kernel that supports dynamic output shapes - Please report an issue to PyTorch"
  },
  {
    "id": "gb0038",
    "title": "GB0038",
    "url": "gb/gb0038.html",
    "content": "Dynamic slicing with Tensor arguments SliceVariable start: {start}, stop: {stop}, step: {step} Creating slices with Tensor arguments is not supported. e.g. `l[:x]`, where `x` is a 1-element tensor. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0039",
    "title": "GB0039",
    "url": "gb/gb0039.html",
    "content": "Dynamo cache limit exceeded Limit type: {limit_type} Dynamo attempted to recompile the code object too many times, exceeding the {limit_type} cache size limit.Giving up on compiling as the compile time tradeoff is likely not worth the performance gain. *No hints provided.*"
  },
  {
    "id": "gb0040",
    "title": "GB0040",
    "url": "gb/gb0040.html",
    "content": "Encountered aliasing during higher order op tracing context Higher order ops do not support aliasing. Found in {source_target.name()} - Replace `return input` with `return input.clone()` to avoid aliasing. - Consider using the debug context to change user code to avoid aliasing. - Please open an issue."
  },
  {
    "id": "gb0041",
    "title": "GB0041",
    "url": "gb/gb0041.html",
    "content": "Encountered input mutation during higher order op tracing context Higher order ops do not support input mutation. Found in {source_target.name()} - Consider using the debug context to change user code to avoid mutation. - Please open an issue."
  },
  {
    "id": "gb0042",
    "title": "GB0042",
    "url": "gb/gb0042.html",
    "content": "Encountered non user function variable during invoke_subgraph HOP tracing str(fn_vt) invoke_subgraph does not support non user function variable - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0043",
    "title": "GB0043",
    "url": "gb/gb0043.html",
    "content": "Encountered non-PT2-compliant op  msg +   + err_epilogue *No hints provided.*"
  },
  {
    "id": "gb0044",
    "title": "GB0044",
    "url": "gb/gb0044.html",
    "content": "Encountered strided NestedTensor in automatic dynamic dim determination  torch.compile does not support strided NestedTensor *No hints provided.*"
  },
  {
    "id": "gb0045",
    "title": "GB0045",
    "url": "gb/gb0045.html",
    "content": "Encountered tensor.is_inference() during tracing  tensor.is_inference() is not supported - This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround."
  },
  {
    "id": "gb0046",
    "title": "GB0046",
    "url": "gb/gb0046.html",
    "content": "Encountered torch.is_inference_mode_enabled during tracing  torch.is_inference_mode_enabled() is not supported - This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround."
  },
  {
    "id": "gb0047",
    "title": "GB0047",
    "url": "gb/gb0047.html",
    "content": "Encountered unconverted argument when attempting to inline func: {func}, arg: {v} An argument to an inlined function was not successfully converted to a VariableTracker. - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0048",
    "title": "GB0048",
    "url": "gb/gb0048.html",
    "content": "Error getting associated real value call_id {self} Dynamo encountered an error while trying to get the associated real value. *No hints provided.*"
  },
  {
    "id": "gb0049",
    "title": "GB0049",
    "url": "gb/gb0049.html",
    "content": "Error when attempting to resolve op packet  str(e) *No hints provided.*"
  },
  {
    "id": "gb0050",
    "title": "GB0050",
    "url": "gb/gb0050.html",
    "content": "Exception with bad expected type str(expected_exc_types) `except ...` has unsupported type {expected_exc_types}. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0051",
    "title": "GB0051",
    "url": "gb/gb0051.html",
    "content": "Exception with non-type expectation str(expected_type) `except ...` expects a non-type: {expected_type}. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0052",
    "title": "GB0052",
    "url": "gb/gb0052.html",
    "content": "Excessive RestartAnalysis() calls  Dynamo attempted to trace the same frame 100+ times. Giving up on compiling as the compile time tradeoff is likely not worth the performance gain. *No hints provided.*"
  },
  {
    "id": "gb0053",
    "title": "GB0053",
    "url": "gb/gb0053.html",
    "content": "FSDP with use_orig_params=False  Dynamo only supports FSDP with use_orig_params=True *No hints provided.*"
  },
  {
    "id": "gb0054",
    "title": "GB0054",
    "url": "gb/gb0054.html",
    "content": "Failed to construct Enum variable value: {value_vt}, allowed enum values: {list(cls_type)} Attempted to construct an Enum value that is non-constant (e.g. int, string) or is not an acceptable value for the Enum. Acceptable values for Enum `{cls_type}`: {list(cls_type)}. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0055",
    "title": "GB0055",
    "url": "gb/gb0055.html",
    "content": "Failed to convert args/kwargs to proxy call_function args: {typestr(*args)} {typestr(*list(kwargs.values()))} Missing `as_proxy()` implementation for some arg/kwarg. *No hints provided.*"
  },
  {
    "id": "gb0056",
    "title": "GB0056",
    "url": "gb/gb0056.html",
    "content": "Failed to mutate tensor data attribute setattr({obj}, {name}, {val}) Dyanmo only supports mutating `.data` of tensor created outside `torch.compile` region - Don't mutate `.data` on this tensor, or move  - the mutation out of `torch.compile` region"
  },
  {
    "id": "gb0057",
    "title": "GB0057",
    "url": "gb/gb0057.html",
    "content": "Failed to raise exception str(exc) Attempted to raise a non-Exception type/value. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0058",
    "title": "GB0058",
    "url": "gb/gb0058.html",
    "content": "Failed to set tensor attribute setattr({obj}, {name}, {val}) Dyanmo doesn't support setting these tensor attributes - Don't mutate attribute '{name}' on tensors, or  - move the mutation out of `torch.compile` region"
  },
  {
    "id": "gb0059",
    "title": "GB0059",
    "url": "gb/gb0059.html",
    "content": "Failed to trace builtin operator builtin {fn.__name__} {arg_types} {has_kwargs} Dynamo does not know how to trace builtin operator `{fn.__name__}` with argument types {real_arg_types} (has_kwargs {has_kwargs}) - Avoid calling builtin `{fn.__name__}` with argument types {real_arg_types}.  - Consider using an equivalent alternative function/method to `{fn.__name__}`. - If you are attempting to call a logging function (e.g. `print`),  - you can try adding it to `torch._dynamo.config.reorderable_logging_functions`. - Please report an issue to PyTorch."
  },
  {
    "id": "gb0060",
    "title": "GB0060",
    "url": "gb/gb0060.html",
    "content": "Failed to trace unittest method function: unittest.TestCase.{name} Dynamo does not know how to trace unittest method `{name}`  - Avoid calling `TestCase.{name}`.  - Please report an issue to PyTorch."
  },
  {
    "id": "gb0061",
    "title": "GB0061",
    "url": "gb/gb0061.html",
    "content": "Failed to unpack object for BUILD_LIST_UNPACK str(seq) {seq} cannot be unpacked into a list for the BUILD_LIST_UNPACK bytecode (`[*x, *y, ...]`). - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0062",
    "title": "GB0062",
    "url": "gb/gb0062.html",
    "content": "Failed to unpack object for UNPACK_EX str(seq) {seq} cannot be unpacked into a list for the UNPACK_EX bytecode. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0063",
    "title": "GB0063",
    "url": "gb/gb0063.html",
    "content": "Failed to unpack object for UNPACK_SEQUENCE str(seq) {seq} cannot be unpacked into a list for the UNPACK_SEQUENCE bytecode (i.e. `a, b, c = d`). - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0064",
    "title": "GB0064",
    "url": "gb/gb0064.html",
    "content": "Fake tensor propagation exception str(e.reason) msg *No hints provided.*"
  },
  {
    "id": "gb0065",
    "title": "GB0065",
    "url": "gb/gb0065.html",
    "content": "Graph break in inlined function  Graph breaks in an inlined call are not supported. *No hints provided.*"
  },
  {
    "id": "gb0066",
    "title": "GB0066",
    "url": "gb/gb0066.html",
    "content": "Graph break under GenericContextWrappingVariable Active generic context managers: {self.active_generic_context_managers} Attempted to graph break in an active context manager(s) that doesn't support graph breaking. - Move the offending context manager(s) to outside the compiled region. - This graph break may have been caused by an earlier graph break. Resolving the earlier graph break may resolve this one."
  },
  {
    "id": "gb0067",
    "title": "GB0067",
    "url": "gb/gb0067.html",
    "content": "HigherOrderOperator: Mutating a variable not in the current scope (SideEffects)  This is not supported. *No hints provided.*"
  },
  {
    "id": "gb0068",
    "title": "GB0068",
    "url": "gb/gb0068.html",
    "content": "Illegal method invocation in strict mode call_method {self} {name} {args} {kwargs} Dynamo currently does not support this method ({name}) invocation in strict mode. *No hints provided.*"
  },
  {
    "id": "gb0069",
    "title": "GB0069",
    "url": "gb/gb0069.html",
    "content": "Import failure module_name: {module_name}, fromlist: {fromlist}, level={level} Failure when attempting to import. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0070",
    "title": "GB0070",
    "url": "gb/gb0070.html",
    "content": "Indexing list with non-scalar tensor call_method {self} {name} {args} {kwargs} Attempted to index list-like object with tensor with > 1 element. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0071",
    "title": "GB0071",
    "url": "gb/gb0071.html",
    "content": "Inline attempt with __self__ str(func) Attempted to inline a function with the `__self__` attribute. Dynamo is expected to decompose method calls into function calls with a `self` argument. *No hints provided.*"
  },
  {
    "id": "gb0072",
    "title": "GB0072",
    "url": "gb/gb0072.html",
    "content": "Inplace op on input tensor  Attempted to trace an inplace view op on input tensor {typestr(self.value)}. - Ensure you do not modify input tensor in place. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0073",
    "title": "GB0073",
    "url": "gb/gb0073.html",
    "content": "Invoking an nn.Module inside a HigherOrderOperator  This is not supported. *No hints provided.*"
  },
  {
    "id": "gb0074",
    "title": "GB0074",
    "url": "gb/gb0074.html",
    "content": "Invoking an nn.Module inside a higher order operator Higher order op name: {self.source_target} This is not supported. *No hints provided.*"
  },
  {
    "id": "gb0075",
    "title": "GB0075",
    "url": "gb/gb0075.html",
    "content": "LOAD_BUILD_CLASS bytecode not supported  Dynamo does not support tracing classes that are defined in the compiled region. - Move the class definition out of the compiled region. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0076",
    "title": "GB0076",
    "url": "gb/gb0076.html",
    "content": "LOAD_FAST_CHECK on uninitialized variable inst.argval Attempted to load uninitialized local variable {inst.argval} - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0077",
    "title": "GB0077",
    "url": "gb/gb0077.html",
    "content": "Length mismatch when unpacking object for UNPACK_SEQUENCE expected length: {inst.argval}, actual: {len(val)} {seq} unpacked to a list for the UNPACK_SEQUENCE bytecode (i.e. `a, b, c = d`) with unexpected length. - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0078",
    "title": "GB0078",
    "url": "gb/gb0078.html",
    "content": "Limitation of `nonstrict_trace {self} msg - make sure definition of {fn_name} is outside  - `torch.compile` region"
  },
  {
    "id": "gb0079",
    "title": "GB0079",
    "url": "gb/gb0079.html",
    "content": "Missing CALL_INTRINSIC_1 handler CALL_INTRINSIC_1 operand: {inst.argval} No handler implemented for CALL_INTRINSIC_1 {inst.argval} instruction. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0080",
    "title": "GB0080",
    "url": "gb/gb0080.html",
    "content": "Missing FakeTensor example value str(node) `FakeTensor` example value was required for {node} but not available. - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0081",
    "title": "GB0081",
    "url": "gb/gb0081.html",
    "content": "Missing attribute when running call_method node  make_error_message(\"attribute not defined\") *No hints provided.*"
  },
  {
    "id": "gb0082",
    "title": "GB0082",
    "url": "gb/gb0082.html",
    "content": "Missing bytecode handler {opname} with args {args} Dynamo does not know how to handle the bytecode instruction `{opname}`. - Do not trace code that produces the `{opname}` bytecode instruction  - (see https://docs.python.org/3/library/dis.html for bytecode semantics). - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0083",
    "title": "GB0083",
    "url": "gb/gb0083.html",
    "content": "Module-level backwards hooks require compiled autograd.   - Enable compiled autograd by setting torch._dynamo.config.compiled_autograd = True."
  },
  {
    "id": "gb0084",
    "title": "GB0084",
    "url": "gb/gb0084.html",
    "content": "Non-constant attribute given to `super().__delattr__()` call_method {self} {name} Dynamo requires the attribute name passed to `super().__delattr__(...)` to be a constant (string). - Ensure the attribute name is a string literal or a constant variable."
  },
  {
    "id": "gb0085",
    "title": "GB0085",
    "url": "gb/gb0085.html",
    "content": "Non-function or method in subclass of torch.autograd.Function call_apply {self} {args} {kwargs} Dynamo requires the `forward` attribute of a `torch.autograd.Function` subclass to be a standard Python function or method. Found type `{type(fn).__name__}` instead. - Ensure the `forward` method is defined as a regular  - function or instance method."
  },
  {
    "id": "gb0086",
    "title": "GB0086",
    "url": "gb/gb0086.html",
    "content": "Not a Python constant guard_as_python_constant {self} Failed to convert {self} into a Python constant. *No hints provided.*"
  },
  {
    "id": "gb0087",
    "title": "GB0087",
    "url": "gb/gb0087.html",
    "content": "NotImplementedError/UnsupportedFakeTensorException when running FX node  make_error_message(e) *No hints provided.*"
  },
  {
    "id": "gb0088",
    "title": "GB0088",
    "url": "gb/gb0088.html",
    "content": "Observed exception str(raised_exception) observed_exn_gb_explanation - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0089",
    "title": "GB0089",
    "url": "gb/gb0089.html",
    "content": "Observed exception (EXCEPT_HANDLER) str(raised_exception) observed_exn_gb_explanation                                 + \" This graph break is unexpected.\" - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0090",
    "title": "GB0090",
    "url": "gb/gb0090.html",
    "content": "Operator does not support running with fake tensors unsupported operator: {cause.func}  - {import_suggestion}see  - https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.64r4npvq0w0 -  for how to fix"
  },
  {
    "id": "gb0091",
    "title": "GB0091",
    "url": "gb/gb0091.html",
    "content": "Read uninitialized cell str(cellvar) Attempted to read a cell variable that has not been populated yet. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0092",
    "title": "GB0092",
    "url": "gb/gb0092.html",
    "content": "Reconstruction failure str(value) Dynamo has no bytecode reconstruction implemented for sourceless variable {value}. - If Dynamo is attempting to trace a return statement and your code is attempting to return a variable  - that Dynamo cannot reconstruct, then remove it from the return statement. - Report an issue to PyTorch if you need reconstrtuction support. Note that objects that don't have  - reconstruction rules may be fundamentally unreconstructable. - This graph break may have been caused by an earlier graph break. Resolving the earlier graph break may resolve this one."
  },
  {
    "id": "gb0093",
    "title": "GB0093",
    "url": "gb/gb0093.html",
    "content": "Reconstruction failure: source.reconstruct not implemented str(source) Dynamo has no bytecode reconstruction implemented for {type(source)} variable {source}. - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0094",
    "title": "GB0094",
    "url": "gb/gb0094.html",
    "content": "SEND with bad type TOS type: {typestr(tos)} Attempted to SEND with unsupported type {typestr(tos)}. *No hints provided.*"
  },
  {
    "id": "gb0095",
    "title": "GB0095",
    "url": "gb/gb0095.html",
    "content": "Set Exception object `__traceback__` attribute to not-`None` call_setattr {self} {name} Dynamo does not support setting the attribute '__traceback__' on tracked exception objects to anything other than None. - Avoid setting '__traceback__' on exception objects  - within traced code, or set it to None."
  },
  {
    "id": "gb0096",
    "title": "GB0096",
    "url": "gb/gb0096.html",
    "content": "Should not compile partial graph (STORE_ATTR)  Dynamo has determined when encountering an unsupported STORE_ATTR instruction (i.e. `obj.attr = val`) that it should not compile the partial graph. *No hints provided.*"
  },
  {
    "id": "gb0097",
    "title": "GB0097",
    "url": "gb/gb0097.html",
    "content": "Side effect on existing deque with limited maxlen  This is not supported. - Don't use a deque with `maxlen` specified."
  },
  {
    "id": "gb0098",
    "title": "GB0098",
    "url": "gb/gb0098.html",
    "content": "Skip calling `torch.compiler.disable()`d function str(self.value) Skip calling function `{self.value}` since it was wrapped with `torch.compiler.disable` (reason: {msg}) - Remove the `torch.compiler.disable` call"
  },
  {
    "id": "gb0099",
    "title": "GB0099",
    "url": "gb/gb0099.html",
    "content": "Skip inlining `torch.compiler.disable()`d function str(func.get_function()) Skip inlining function {func.get_function()} since it was wrapped with `torch.compiler.disable` (reason: {msg}) - Remove the `torch.compiler.disable` call"
  },
  {
    "id": "gb0100",
    "title": "GB0100",
    "url": "gb/gb0100.html",
    "content": "Storing Tensor hook handle in globals name This is not supported. *No hints provided.*"
  },
  {
    "id": "gb0101",
    "title": "GB0101",
    "url": "gb/gb0101.html",
    "content": "Storing Tensor hook handle in globals (inline call) inst.argval This is not supported. *No hints provided.*"
  },
  {
    "id": "gb0102",
    "title": "GB0102",
    "url": "gb/gb0102.html",
    "content": "Strict mode banned op var_getattr {self} {name} Getattr invocation '{name}' in strict mode is not supported. - Remove `{name}` from the list of banned ops by  - setting `torch._dynamo.config._autograd_backward_strict_mode_banned_ops`."
  },
  {
    "id": "gb0103",
    "title": "GB0103",
    "url": "gb/gb0103.html",
    "content": "Tensor subclass overridden method call {name} `torch.compile` currently can't trace this - Avoid calling {name} of tensor subclass in torch.compile region - Renaming method `{name}` of type {self.class_type} - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0104",
    "title": "GB0104",
    "url": "gb/gb0104.html",
    "content": "Tensor with grad_fn() var_getattr {self} grad_fn Dynamo does not support tracing tensors with a grad_fn directly. *No hints provided.*"
  },
  {
    "id": "gb0105",
    "title": "GB0105",
    "url": "gb/gb0105.html",
    "content": "Tensor.numpy() with trace_numpy=False call_method {self} numpy `Tensor.numpy()` was called, but the `trace_numpy` configuration was manually disabled. - Set `torch._dynamo.config.trace_numpy = True` to allow  - Dynamo to trace through NumPy."
  },
  {
    "id": "gb0106",
    "title": "GB0106",
    "url": "gb/gb0106.html",
    "content": "Tensor.numpy() without NumPy installed call_method {self} numpy `Tensor.numpy()` was called, but the NumPy library is not available in the current environment. - Ensure NumPy is installed in your Python environment."
  },
  {
    "id": "gb0107",
    "title": "GB0107",
    "url": "gb/gb0107.html",
    "content": "Tensor.random_ op Tensor.{name}(args={args}, kwargs={kwargs}) This is currently not supported. - Use the out-of-place version of this op - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0108",
    "title": "GB0108",
    "url": "gb/gb0108.html",
    "content": "Tensor.retain_grad() with AOTDispatcher var_getattr {self} retain_grad `Tensor.retain_grad()` does not work with AOTDispatcher. *No hints provided.*"
  },
  {
    "id": "gb0109",
    "title": "GB0109",
    "url": "gb/gb0109.html",
    "content": "Tensor.tolist() with non-integer tensor call_method {self} to_list Dynamo currently does not support tracing `tolist()` on non-integer tensors. - Ensure the input tensor to `tolist()` is an integer  - type (e.g., int8, int16, int32, int64)."
  },
  {
    "id": "gb0110",
    "title": "GB0110",
    "url": "gb/gb0110.html",
    "content": "Tensor.uniform_ op called with `from` keyword Tensor.{name}(args={args}, kwargs={kwargs}) This is currently not supported. - Avoid using the `from` keyword. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0111",
    "title": "GB0111",
    "url": "gb/gb0111.html",
    "content": "TypeError from user code call_function({self.value}, {args}, {kwargs}) msg - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0112",
    "title": "GB0112",
    "url": "gb/gb0112.html",
    "content": "TypeError when making fake tensor call TypeError {node.target}: {cause}  *No hints provided.*"
  },
  {
    "id": "gb0113",
    "title": "GB0113",
    "url": "gb/gb0113.html",
    "content": "Unable to resolve super getattr  Dynamo failed to trace attribute `{name}` accessed via `super()` (for type `{self.typevar}` and object `{self.objvar}`) because the resolved attribute type is not supported. - Ensure the attribute exists in the parent class. - Check the arguments passed to `super()`."
  },
  {
    "id": "gb0114",
    "title": "GB0114",
    "url": "gb/gb0114.html",
    "content": "Unexpected failure during itertools.accumulate() iteration call_function {self} {args} {kwargs} Unexpected failure in invoking function during accumulate. Failed running func {func}({item}{acc}) - This graph break may be difficult to debug. Please report an issue to PyTorch for assistance."
  },
  {
    "id": "gb0115",
    "title": "GB0115",
    "url": "gb/gb0115.html",
    "content": "Unexpected failure during itertools.groupby() iteration call_function {self} {args} {kwargs} Unexpected failure in invoking function during groupby - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0116",
    "title": "GB0116",
    "url": "gb/gb0116.html",
    "content": "Unexpected type in sourceless builder {value_type.__module__}.{value_type.__qualname__} SourcelessBuilder.create does not know how to wrap {value_type} - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0117",
    "title": "GB0117",
    "url": "gb/gb0117.html",
    "content": "Unhandled args for method call_method {self} {name} {args} {kwargs} Dynamo encountered an error while calling the method `{name}`. *No hints provided.*"
  },
  {
    "id": "gb0118",
    "title": "GB0118",
    "url": "gb/gb0118.html",
    "content": "Unimplemented next() call next({self}) This abstract method must be implemented - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0119",
    "title": "GB0119",
    "url": "gb/gb0119.html",
    "content": "Uninitialized nn.Module typestr(value) Attempted to trace an uninitialized nn.Module of type {typestr(value)}. - Ensure your nn.Module instance has called `super().__init__()`. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0120",
    "title": "GB0120",
    "url": "gb/gb0120.html",
    "content": "Unreachable sub-generator code  Should only be encountered while implementing generator support. *No hints provided.*"
  },
  {
    "id": "gb0121",
    "title": "GB0121",
    "url": "gb/gb0121.html",
    "content": "UnspecializedNNModuleVariable missing method call_method: {self} {name} {args} {kwargs} Dynamo does not support tracing method {name} of nn.Module {self.value} - Dynamo does not really define unspecialized nn.Module very well. - This graph break may be difficult to debug. Please report an issue to PyTorch for assistance."
  },
  {
    "id": "gb0122",
    "title": "GB0122",
    "url": "gb/gb0122.html",
    "content": "Unsupported SourceType MutationType.__init__ {self} {typ} Dynamo does not support the type `{typ}` - This branch is not supposed to be reachable. - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0123",
    "title": "GB0123",
    "url": "gb/gb0123.html",
    "content": "Unsupported Tensor.backward() call call_method {self} backward {args} {kwargs} Dynamo currently does not support tracing `Tensor.backward()`. - This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround."
  },
  {
    "id": "gb0124",
    "title": "GB0124",
    "url": "gb/gb0124.html",
    "content": "Unsupported Tensor.item() call with capture_scalar_outputs=False call_method {self} item {args} {kwargs} Dynamo does not support tracing `Tensor.item()` with config.capture_scalar_outputs=False. - Set `torch._dynamo.config.capture_scalar_outputs = True`  - or `export TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1`  - to include these operations in the captured graph."
  },
  {
    "id": "gb0125",
    "title": "GB0125",
    "url": "gb/gb0125.html",
    "content": "Unsupported Tensor.requires_grad_() call call_method {self} requires_grad_ Dynamo does not support changes to a Tensor's `requires_grad` through calling `requires_grad_()`. *No hints provided.*"
  },
  {
    "id": "gb0126",
    "title": "GB0126",
    "url": "gb/gb0126.html",
    "content": "Unsupported Tensor.resize_() call call_method {self} resize_ {args} {kwargs} Dynamo currently does not support tracing `Tensor.resize_()`. *No hints provided.*"
  },
  {
    "id": "gb0127",
    "title": "GB0127",
    "url": "gb/gb0127.html",
    "content": "Unsupported Tensor.resize_as_() call call_method {self} resize_as_ {args} {kwargs} Dynamo currently does not support tracing `Tensor.resize_as_()`. *No hints provided.*"
  },
  {
    "id": "gb0128",
    "title": "GB0128",
    "url": "gb/gb0128.html",
    "content": "Unsupported Tensor.set_() call call_method {self} set_ {args} {kwargs} Dynamo currently does not support tracing `Tensor.set_()` overloads that include more than one argument. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0129",
    "title": "GB0129",
    "url": "gb/gb0129.html",
    "content": "Unsupported Tensor.sparse_resize_() call call_method {self} sparse_resize_ {args} {kwargs} Dynamo currently does not support tracing `Tensor.sparse_resize_()`. *No hints provided.*"
  },
  {
    "id": "gb0130",
    "title": "GB0130",
    "url": "gb/gb0130.html",
    "content": "Unsupported Tensor.sparse_resize_and_clear_() call call_method {self} sparse_resize_and_clear_ {args} {kwargs} Dynamo currently does not support tracing `Tensor.sparse_resize_and_clear_()`. *No hints provided.*"
  },
  {
    "id": "gb0131",
    "title": "GB0131",
    "url": "gb/gb0131.html",
    "content": "Unsupported __setitem__/__setattr__ inline attempt code name: {code.co_name}, args: {args} Attempted to inline {code.co_name} where first argument (self) is not a user-defined object. *No hints provided.*"
  },
  {
    "id": "gb0132",
    "title": "GB0132",
    "url": "gb/gb0132.html",
    "content": "Unsupported `func` in itertools.accumulate call_function {self} {args} {kwargs} Dynamo does not know how to get the function to use for itertools.accumulate. itertools.accumulate expects the `func` as the second argument or as a keyword argument. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0133",
    "title": "GB0133",
    "url": "gb/gb0133.html",
    "content": "Unsupported arguments for itertools.accumulate call_function {self} {args} {kwargs} Dynamo does not know how to trace itertools.accumulate with args: {args} and kwargs: {kwargs}. itertools.accumulate expects an iterable, an optional binary function for accumulation, and an optional initial value to set the starting state. - Make sure the arguments to itertools.accumulate are correct. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0134",
    "title": "GB0134",
    "url": "gb/gb0134.html",
    "content": "Unsupported arguments for itertools.groupby call_function {self} {args} {kwargs} Dynamo does not know how to trace itertools.groupby with args: {args} and kwargs: {kwargs}. itertools.groupby expects an iterable to group and an optional key function to determine groupings. - Make sure the arguments to itertools.groupby are correct. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0135",
    "title": "GB0135",
    "url": "gb/gb0135.html",
    "content": "Unsupported attribute assignment on Exception object call_setattr {self} {name} Dynamo does not support setting the attribute '{name}' on tracked exception objects. Only `__context__`, `__cause__`, `__suppress_context__`, and `__traceback__` are supported. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0136",
    "title": "GB0136",
    "url": "gb/gb0136.html",
    "content": "Unsupported attribute for range() object var_getattr {self} {name} Expected attribute to be one of {','.join(fields)} but got {name} - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0137",
    "title": "GB0137",
    "url": "gb/gb0137.html",
    "content": "Unsupported attribute for slice() object var_getattr {self} {name} Expected attribute to be one of {','.join(fields)} but got {name} - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0138",
    "title": "GB0138",
    "url": "gb/gb0138.html",
    "content": "Unsupported autograd.Function context `save_for_backward` call_method {self} {name} Dynamo requires the `saved_tensors` attribute to be initialized on the `autograd.Function` context object. - Ensure that the `saved_tensors` attribute is properly  - initialized before calling `save_for_backward`.  - `save_for_backward` only supported on a newly constructed `torch.autograd.function.FunctionCtx`."
  },
  {
    "id": "gb0139",
    "title": "GB0139",
    "url": "gb/gb0139.html",
    "content": "Unsupported autograd.Function context method call_method {self} {name} Dynamo does not support calling the method `{name}` on `autograd.Function` context objects. Supported methods are `__setattr__`, `save_for_backward` and `mark_non_differentiable`. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0140",
    "title": "GB0140",
    "url": "gb/gb0140.html",
    "content": "Unsupported autograd.Function method call_method {self} {name} Dynamo does not support calling the method `{name}` directly on the `torch.autograd.Function` instance. Supported methods include `apply`, `backward`, static methods, and class methods. - Ensure the method is decorated with `@staticmethod`  - or `@classmethod` if it's meant to be called on the class."
  },
  {
    "id": "gb0141",
    "title": "GB0141",
    "url": "gb/gb0141.html",
    "content": "Unsupported call_id() without source call_id {self} call_id() not supported for sourceless TensorVariable. *No hints provided.*"
  },
  {
    "id": "gb0142",
    "title": "GB0142",
    "url": "gb/gb0142.html",
    "content": "Unsupported context manager Attempted SETUP_WITH/BEFORE_WITH on {ctx} Dynamo does not know how to enter a `{ctx.python_type_name()}` context manager. - Avoid using the unsupported context manager. - If the context manager seems like it should be supported (e.g. torch.set_grad_enabled), then  - it may be the case that it was created outside the compiled region, which Dynamo does not support.  - Supported context managers can cross graph break boundaries only if they are local non-closure  - variables, or are intermediate values. - File an issue to PyTorch. Simple context managers can potentially be supported,  - but note that context managers can't be supported in general"
  },
  {
    "id": "gb0143",
    "title": "GB0143",
    "url": "gb/gb0143.html",
    "content": "Unsupported conversion for slice assignment call_method {self} {name} {args} Missing dynamo support for converting {value} into a list for slice assignment. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0144",
    "title": "GB0144",
    "url": "gb/gb0144.html",
    "content": "Unsupported custom jvp call_apply {self} {args} {kwargs} Dynamo does not support tracing `torch.autograd.Function` subclasses that define a custom `jvp` method. - Remove the custom `jvp` method if possible. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0145",
    "title": "GB0145",
    "url": "gb/gb0145.html",
    "content": "Unsupported custom vjp call_apply {self} {args} {kwargs} Dynamo does not support tracing `torch.autograd.Function` subclasses that define a custom `vjp` method. - Remove the custom `vjp` method if possible. - Use standard `backward` instead if applicable. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0146",
    "title": "GB0146",
    "url": "gb/gb0146.html",
    "content": "Unsupported event method str(name) Dynamo doesn't support tracing the {method_name} method. We currently support wait, record, synchronize, and query. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0147",
    "title": "GB0147",
    "url": "gb/gb0147.html",
    "content": "Unsupported function call call_function {self} {args} {kwargs} Dynamo does not know how to trace the function `{self.debug_repr()}` - Avoid calling `{self.debug_repr()}` in your code. - Please report an issue to PyTorch."
  },
  {
    "id": "gb0148",
    "title": "GB0148",
    "url": "gb/gb0148.html",
    "content": "Unsupported function call (delayed) source: {self.source} Dynamo determined that a graph break should occur when calling `{self.source.name()}`. Reason: {self.msg} *No hints provided.*"
  },
  {
    "id": "gb0149",
    "title": "GB0149",
    "url": "gb/gb0149.html",
    "content": "Unsupported functorch tracing attempt  msg *No hints provided.*"
  },
  {
    "id": "gb0150",
    "title": "GB0150",
    "url": "gb/gb0150.html",
    "content": "Unsupported hasattr call call_obj_hasattr {self} {name} Dynamo does not know how to trace the function `{self.debug_repr()}` - Avoid calling `hasattr({self.__class__.__name__}, {name})` in your code. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0151",
    "title": "GB0151",
    "url": "gb/gb0151.html",
    "content": "Unsupported inspect call inspect_parameter_names {self} Dynamo does not know how to trace the function `{self.debug_repr()}` *No hints provided.*"
  },
  {
    "id": "gb0152",
    "title": "GB0152",
    "url": "gb/gb0152.html",
    "content": "Unsupported key type for itertools.groupby call_function {self} {args} {kwargs} Dynamo does not know how to trace itertools.groupby with key type: {str(type(key))}. We only support grouping keys that are constants (int, float, str, etc.) - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0153",
    "title": "GB0153",
    "url": "gb/gb0153.html",
    "content": "Unsupported key type for nn.Module.__getitem__ call_method: {self} {name} {args} {kwargs} Dynamo does not support getitem on `nn.Module` with non-constant key. *No hints provided.*"
  },
  {
    "id": "gb0154",
    "title": "GB0154",
    "url": "gb/gb0154.html",
    "content": "Unsupported kwargs for itertools.accumulate call_function {self} {args} {kwargs} Expected kwargs: 'initial', 'func', but got {','.join(set(kwargs.keys()) - {'initial', 'func'})} - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0155",
    "title": "GB0155",
    "url": "gb/gb0155.html",
    "content": "Unsupported kwargs for itertools.groupby call_function {self} {args} {kwargs} Expected kwargs: 'key', but got {','.join(set(kwargs.keys()) - {'key'})} - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0156",
    "title": "GB0156",
    "url": "gb/gb0156.html",
    "content": "Unsupported method call call_method {self} {name} {args} {kwargs} Dynamo does not know how to trace method `{name}` of class `{self.python_type_name()}` *No hints provided.*"
  },
  {
    "id": "gb0157",
    "title": "GB0157",
    "url": "gb/gb0157.html",
    "content": "Unsupported ndarray attribute access var_getattr {self} {name} Dynamo currently does not support tracing `ndarray.{name}`. *No hints provided.*"
  },
  {
    "id": "gb0158",
    "title": "GB0158",
    "url": "gb/gb0158.html",
    "content": "Unsupported ndarray method call call_method {self} {name} {args} {kwargs} `ndarray.{name}()` is not modelled in `torch._numpy`. *No hints provided.*"
  },
  {
    "id": "gb0159",
    "title": "GB0159",
    "url": "gb/gb0159.html",
    "content": "Unsupported ndarray.__version__ access var_getattr {self} {name} Dynamo currently does not support tracing `ndarray.{name}`. *No hints provided.*"
  },
  {
    "id": "gb0160",
    "title": "GB0160",
    "url": "gb/gb0160.html",
    "content": "Unsupported next() call next({self}) Dynamo does not know how to trace calling `next()` on variable `{self}`. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0161",
    "title": "GB0161",
    "url": "gb/gb0161.html",
    "content": "Unsupported nn.Module attribute type nn.Module subclass: {typestr(base)}, name: {name}, attribute type: {typestr(subobj)} Dynamo does not support tracing nn.Module attributes of type `{typestr(subobj)}` - Refactor your code so that `{name}` (type `{typestr(subobj)}`) is not an attribute of `{typestr(base)}` - Currently supported attribute types are methods, classmethods, staticmethods,  - properties, constants, and tensors. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0162",
    "title": "GB0162",
    "url": "gb/gb0162.html",
    "content": "Unsupported super().__init__() call call_method {self} {name} {args} {kwargs} Dynamo encountered a super().__init__() call on {objvar} that resolved to a `torch.nn.Module.__init__()` call that we cannot trace. - This graph break may be difficult to debug. Please report an issue to PyTorch for assistance."
  },
  {
    "id": "gb0163",
    "title": "GB0163",
    "url": "gb/gb0163.html",
    "content": "Unsupported tensor subclass attribute access {name} `torch.compile` currently can't trace this - Avoid accessing {name} of tensor subclass in torch.compile region - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0164",
    "title": "GB0164",
    "url": "gb/gb0164.html",
    "content": "Unsupported tensor subclass overridden attribute access {name} `torch.compile` only support tracing certain types of overridden tensor subclass attributes - Avoid accessing {name} of tensor subclass in torch.compile region - Renaming attribute `{name}` of type {self.class_type} - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0165",
    "title": "GB0165",
    "url": "gb/gb0165.html",
    "content": "Unsupported torch._C._ImperativeEngine method call_method {self} {name} Dynamo only supports the `queue_callback` method on a torch._C._ImperativeEngine instance, but found: `{name}`. *No hints provided.*"
  },
  {
    "id": "gb0166",
    "title": "GB0166",
    "url": "gb/gb0166.html",
    "content": "Unsupported torch._C._ImperativeEngine.queue_callback() call_method {self} {name} queue_callback() is only supported when Compiled Autograd is enabled with fullgraph=True. *No hints provided.*"
  },
  {
    "id": "gb0167",
    "title": "GB0167",
    "url": "gb/gb0167.html",
    "content": "Variadic function call with bad args/kwargs type args type: {typestr(argsvars)}, kwargs type: {typestr(kwargsvars)} Expected args to be a list and kwargs to be a dict - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0168",
    "title": "GB0168",
    "url": "gb/gb0168.html",
    "content": "Variadic function call with bad flags flags: {inst.argval} Attempted to call a variadic function (CALL_FUNCTION_EX) with bad flags {inst.argval} - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0169",
    "title": "GB0169",
    "url": "gb/gb0169.html",
    "content": "Write to immutable cell cellvar: {cellvar}, value: {value} Dynamo doesn't support writing to immutable/sourceless cell variables. - This graph break may be difficult to debug. Please report an issue to PyTorch for assistance."
  },
  {
    "id": "gb0170",
    "title": "GB0170",
    "url": "gb/gb0170.html",
    "content": "Data-dependent branching attempted to jump with {value} _explanation - Use `torch.cond` to express dynamic control flow. - This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround."
  },
  {
    "id": "gb0171",
    "title": "GB0171",
    "url": "gb/gb0171.html",
    "content": "assert with non-string message str(args) Dynamo only supports asserts with string messages - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0172",
    "title": "GB0172",
    "url": "gb/gb0172.html",
    "content": "async_op=True for distributed collectives {self.fn}, args={args}, kwargs={kwargs} `torch.compile` doesn't support `async_op=True for {self.fn} - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0173",
    "title": "GB0173",
    "url": "gb/gb0173.html",
    "content": "backward_state does not support export  Compiled autograd doesn't work with `torch.export`. *No hints provided.*"
  },
  {
    "id": "gb0174",
    "title": "GB0174",
    "url": "gb/gb0174.html",
    "content": "bad args to builtin cast() got args {args} {kwargs} Dynamo expects exactly 2 args to builtin cast(). - Ensure your call to cast() has exactly 2 arguments."
  },
  {
    "id": "gb0175",
    "title": "GB0175",
    "url": "gb/gb0175.html",
    "content": "builtin isinstance() cannot determine type of argument isinstance({arg}, {isinstance_type}) Dynamo doesn't have a rule to determine the type of argument {arg} - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0176",
    "title": "GB0176",
    "url": "gb/gb0176.html",
    "content": "call_id() without associated real value call_id {self} Dynamo could not find an associated real value for the tensor. *No hints provided.*"
  },
  {
    "id": "gb0177",
    "title": "GB0177",
    "url": "gb/gb0177.html",
    "content": "can't handle functions not implemented in python  {fn} Dynamo can only handle functions defined in python - Move usage of this function out of `torch.compile` region - Avoid using `tensor.is_inference()` and `torch.is_inference_mode_enabled()` in your compile code. This is primarily used in conjunction with `torch.inference_mode`. Consider using `torch.no_grad` instead because `torch.no_grad` leads to same improvements as `inference_mode` when `torch.compile` is used."
  },
  {
    "id": "gb0178",
    "title": "GB0178",
    "url": "gb/gb0178.html",
    "content": "constant fold exception attempted to run function {fn} with arguments {args} Encountered exception when attempting to constant fold. - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0179",
    "title": "GB0179",
    "url": "gb/gb0179.html",
    "content": "copy.deepcopy() copy.deepcopy({x}) Dynamo does not support copy.deepcopy() - Avoid calling copy.deepcopy() - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0180",
    "title": "GB0180",
    "url": "gb/gb0180.html",
    "content": "dataclass fields failure obj: {obj}; variable type: {type(obj)} Dataclass fields handling fails for {obj}. Expected it to be a user-defined object. *No hints provided.*"
  },
  {
    "id": "gb0181",
    "title": "GB0181",
    "url": "gb/gb0181.html",
    "content": "dtype mismatch between tensor and its gradient tensor dtype: {value.dtype}; grad dtype: {safe_grad(value).dtype} Inconsistent dtype between tensor and its gradient. This can happen in FSDP and crashes meta tensor creation. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0182",
    "title": "GB0182",
    "url": "gb/gb0182.html",
    "content": "failed to broadcast when attempting Tensor comparison op {op.__name__}({left}, {right}) Dynamo was unable to broad cast the arguments {left}, {right} when attempting to trace the comparison op {op.__name__}. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0183",
    "title": "GB0183",
    "url": "gb/gb0183.html",
    "content": "failed to call dict.fromkeys() {user_cls.__name__}.fromkeys(): {args} {kwargs} Failed to call {user_cls.__name__}.fromkeys() because arguments could not be automatically converted to a list, or some dict key is not hashable. - Manually convert the argument to a list. - Ensure all keys are hashable."
  },
  {
    "id": "gb0184",
    "title": "GB0184",
    "url": "gb/gb0184.html",
    "content": "failed to call str() on user defined object str(arg) User defined object has no __str__ or __repr__ method - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0185",
    "title": "GB0185",
    "url": "gb/gb0185.html",
    "content": "failed to convert numpy.ndarray to Tensor str(value) Exception encountered when attempting to convert numpy.ndarray to Tensor *No hints provided.*"
  },
  {
    "id": "gb0186",
    "title": "GB0186",
    "url": "gb/gb0186.html",
    "content": "functools.partial() with non-literal keyword non-literal keyword: {k} functools.partial() expects literal/string keywords - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0187",
    "title": "GB0187",
    "url": "gb/gb0187.html",
    "content": "functools.wraps {fn} `torch.compile` can't trace `functools.wraps` on functions defined outside the compile region - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0188",
    "title": "GB0188",
    "url": "gb/gb0188.html",
    "content": "getattr with no source var_getattr {self} {name} Dynamo does not know how to access an attribute on an `nn.Module` instance that lacks a source. This is usually an internal error in Dynamo. - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0189",
    "title": "GB0189",
    "url": "gb/gb0189.html",
    "content": "getattr() on nn.Module with pending mutation getattr({obj}, {name}, {default}) Intentionally graph breaking on getattr() on a nn.Module with a pending mutation *No hints provided.*"
  },
  {
    "id": "gb0190",
    "title": "GB0190",
    "url": "gb/gb0190.html",
    "content": "getattr() with non-constant name argument getattr({obj}, {name_var}, {default}) getattr() with non-constant name argument is not supported - Ensure the name argument of getattr() is a string"
  },
  {
    "id": "gb0191",
    "title": "GB0191",
    "url": "gb/gb0191.html",
    "content": "id() with unsupported args str(args) Dynamo doesn't know how to trace id() call with args {args} - Supported args are Tensors, and functions/nn.Modules/user-defined objects  - from outside the compiled region. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0192",
    "title": "GB0192",
    "url": "gb/gb0192.html",
    "content": "input iterator to itertools.cycle has too many items next({self}) Has reached internal Dynamo max iterator limit: {MAX_ITERATOR_LIMIT} *No hints provided.*"
  },
  {
    "id": "gb0193",
    "title": "GB0193",
    "url": "gb/gb0193.html",
    "content": "invalid call to builtin op handler invalid args to {self_handler}: {args} {kwargs} Encountered TypeError when trying to handle op {fn.__name__} - This graph break may be difficult to debug. Please report an issue to PyTorch for assistance."
  },
  {
    "id": "gb0194",
    "title": "GB0194",
    "url": "gb/gb0194.html",
    "content": "isinstance() called on user defined object with C extensions isinstance({arg}, {isinstance_type}) User-defined object with C extensions can have torch.Tensor attributes; intentionally graph breaking. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0195",
    "title": "GB0195",
    "url": "gb/gb0195.html",
    "content": "issubclass() with non-constant arguments issubclass({left_ty}, {right_ty}) issubclass() with non-constant arguments not supported. - Make sure your arguments are types. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0196",
    "title": "GB0196",
    "url": "gb/gb0196.html",
    "content": "key not found in dict Key {arg.value} msg - Check if the key exists in the dictionary before accessing it. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0197",
    "title": "GB0197",
    "url": "gb/gb0197.html",
    "content": "list elements are pointing to the list itself  Dynamo does not support lists whose items reference to itself - Avoid using self referential list"
  },
  {
    "id": "gb0198",
    "title": "GB0198",
    "url": "gb/gb0198.html",
    "content": "mapping proxy affected by dictionary mutation Source: {self.source}, Dict mutation detected msg - Avoid modifying dictionaries that might be referenced by mapping proxy objects - Or avoid using the mapping proxy objects after modifying its underlying dictionary"
  },
  {
    "id": "gb0199",
    "title": "GB0199",
    "url": "gb/gb0199.html",
    "content": "mapping proxy cannot be reconstructed Source: {self.source} msg - Use a mapping proxy constructed in the same `torch.compile` region. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0200",
    "title": "GB0200",
    "url": "gb/gb0200.html",
    "content": "missing BUILD_SET handler  Missing BUILD_SET bytecode handler (for testing purposes). *No hints provided.*"
  },
  {
    "id": "gb0201",
    "title": "GB0201",
    "url": "gb/gb0201.html",
    "content": "namedtuple construction args={args}, kwargs={kwargs} `torch.compile` only support certain input types for namedtuple - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0202",
    "title": "GB0202",
    "url": "gb/gb0202.html",
    "content": "non-const argument in nn.Module method call_method: {self} {name} {args} {kwargs} Dynamo does not support calling method `{name}` of ``nn.Module`` {module} with non-constant arguments. *No hints provided.*"
  },
  {
    "id": "gb0203",
    "title": "GB0203",
    "url": "gb/gb0203.html",
    "content": "non-const keys in dict_keys non-const keys: {[k for k in value if not ConstantVariable.is_literal(k)]} Dynamo expects dict_keys keys to be constants. - Ensure your dict_keys keys are constants (e.g. int, float, strings)"
  },
  {
    "id": "gb0204",
    "title": "GB0204",
    "url": "gb/gb0204.html",
    "content": "non-const keys in mappingproxy non-const keys: {[k for k in value.keys() if not ConstantVariable.is_literal(k)]} Dynamo expects mappingproxy keys to be constants. - Ensure your mappingproxy keys are constants (e.g. int, float, strings)"
  },
  {
    "id": "gb0205",
    "title": "GB0205",
    "url": "gb/gb0205.html",
    "content": "proxy not set as_proxy {self} Dynamo requires the autograd.Function context to be initialized with a proxy. - This is likely to be a Dynamo bug. Please report an issue to PyTorch."
  },
  {
    "id": "gb0206",
    "title": "GB0206",
    "url": "gb/gb0206.html",
    "content": "setattr() on Tensor.requires_grad setattr({obj}, {name}, {val}) setattr() on Tensor.requires_grad not supported. Mutating requires_grad can introduce a new leaf from non-leaf or vice versa in the middle of the graph, which AOTAutograd does not currently know how to handle. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0207",
    "title": "GB0207",
    "url": "gb/gb0207.html",
    "content": "sort with non-constant keys str(first_non_constant_key) Cannot perform sort with non-constant key. First non-constant key type: {python_type}. Most notably, we cannot sort with Tensor or SymInt keys, but we can sort ints. - Use something else as the key."
  },
  {
    "id": "gb0208",
    "title": "GB0208",
    "url": "gb/gb0208.html",
    "content": "torch.* op returned non-Tensor example_value type: {typestr(example_value)}; op: {proxy.node.op}; target: {proxy.node.target} torch.* ops that return a non-Tensor cannot be traced into the Dynamo FX graph output *No hints provided.*"
  },
  {
    "id": "gb0209",
    "title": "GB0209",
    "url": "gb/gb0209.html",
    "content": "torch.autograd._unsafe_preserve_version_counter escaped from compiled region str(self) Dynamo doesn't support compiling a region that returns a torch.autograd._unsafe_preserve_version_counter context manager. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0210",
    "title": "GB0210",
    "url": "gb/gb0210.html",
    "content": "torch.distributed package is not available!  The PyTorch package doesn't include torch.distributed when building from source. - Set USE_DISTRIBUTED=1 to enable it when building PyTorch from source."
  },
  {
    "id": "gb0211",
    "title": "GB0211",
    "url": "gb/gb0211.html",
    "content": "torch.nn.Module with a non-function custom __getattr__ var_getattr {self} {name} Dynamo detected a nn.Module object with a custom `__getattr__` method, but this method is not a standard Python function (e.g., it might be implemented in C/C++). Dynamo cannot currently trace into such non-standard `__getattr__` methods. - Avoid using objects with non-standard __getattr__ methods  - within the compiled region. If possible, implement  - __getattr__ as a standard Python function. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0212",
    "title": "GB0212",
    "url": "gb/gb0212.html",
    "content": "torch.profiler object escaped from compiled region str(self) Dynamo doesn't support compiling a region that returns a torch.profiler context manager. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0213",
    "title": "GB0213",
    "url": "gb/gb0213.html",
    "content": "unimplemented builtin op on tensor arguments partial tensor op: {self} {args} {kwargs} Dynamo does not know how to trace builtin operator {self.fn} with tensor arguments - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0214",
    "title": "GB0214",
    "url": "gb/gb0214.html",
    "content": "unsupported SymNode comparison op {op.__name__}({left}, {right}) Dynamo does not support the comparison op {op.__name__} with SymNode arguments {left}, {right} - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0215",
    "title": "GB0215",
    "url": "gb/gb0215.html",
    "content": "unsupported Tensor comparison op {op.__name__}({left}, {right}) Dynamo does not support the comparison op {op.__name__} with Tensor arguments {left}, {right} - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0216",
    "title": "GB0216",
    "url": "gb/gb0216.html",
    "content": "unsupported grid type for triton hop check_grid grid type = {type(grid)} `torch.compile` only supports list-like grid for check_grid - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0217",
    "title": "GB0217",
    "url": "gb/gb0217.html",
    "content": "unsupported hasattr operation Class {self.user_cls} msg - Consider using a regular dictionary instead - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0218",
    "title": "GB0218",
    "url": "gb/gb0218.html",
    "content": "unsupported index(Tensor)  Dynamo does not support tracing builtin index() on a Tensor *No hints provided.*"
  },
  {
    "id": "gb0219",
    "title": "GB0219",
    "url": "gb/gb0219.html",
    "content": "Backend compiler exception Backend: {name} Exception:{str(e)} Traceback: {self.root_tx.format_frame_summary()} Backend compiler `{name}` failed with {str(e)}. Adding a graph break. - Report an issue to the backend compiler repo."
  },
  {
    "id": "gb0220",
    "title": "GB0220",
    "url": "gb/gb0220.html",
    "content": "Failed to mutate tensor data attribute to different dtype setattr({obj}, {name}, {val}) Dyanmo only supports mutating `.data` of tensor to a new one with the same dtype - Don't mutate `.data` on this tensor, or move  - the mutation out of `torch.compile` region"
  },
  {
    "id": "gb0221",
    "title": "GB0221",
    "url": "gb/gb0221.html",
    "content": "non-generator contextlib.contextmanager str(self.vt.get_code()) Cannot compile function decorated with `@contextlib.contextmanager` that is not a generator, i.e. does not use `yield` - Use `yield` in the function body instead of `return`. - Remove the `@contextlib.contextmanager` decorator."
  },
  {
    "id": "gb0222",
    "title": "GB0222",
    "url": "gb/gb0222.html",
    "content": "Attempted to wrap a set with tensors Python set containing torch.Tensor elements Dynamo cannot trace sets of tensors. To get a stable ordering, Dynamo needs to convert the set into a list and the order might not be stable if the set contains tensors. - Use a dictionary where the keys are tensors. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0223",
    "title": "GB0223",
    "url": "gb/gb0223.html",
    "content": "torch.compile call with > 1 args args={args}, kwargs={kwargs} Attempted to call `torch.compile` with > 1 args. Dynamo does not support this. - Remove the torch.compile call or its additional args. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues. Example code that causes the graph break is:  ```python import torch  def g(x):     return x + 1  @torch.compile(backend=\"eager\")  def f(x):     return torch.compile(g, False)(x)  f(torch.randn(10, 10)) ```"
  },
  {
    "id": "gb0224",
    "title": "GB0224",
    "url": "gb/gb0224.html",
    "content": "Attempted to call torch in-graph function on only torch.SymInt arguments fn={self.value}, args={args}, kwargs={kwargs} Attempted to call {str(self.value)} (that should be put in the FX graph) on only torch.SymInt arguments. Dynamo does not support this. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0225",
    "title": "GB0225",
    "url": "gb/gb0225.html",
    "content": "Attempted to use tensor creation function with requires_grad=True fn={self.value}, args={args}, kwargs={kwargs} Dynamo does not support this. - Create the tensor outside the compiled region. - Do not set `requires_grad=True`. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0226",
    "title": "GB0226",
    "url": "gb/gb0226.html",
    "content": "`torch.nn.Parameter()` with unsupported data type data={data} Called `torch.nn.Parameter()` with non-Tensor argument. - Ensure the argument to `torch.nn.Parameter()` is a `torch.Tensor`. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0227",
    "title": "GB0227",
    "url": "gb/gb0227.html",
    "content": "Attempted to use torch.nn.Parameter constructor with tensor subclass str(data) Dynamo does not support this. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0228",
    "title": "GB0228",
    "url": "gb/gb0228.html",
    "content": "`torch.nn.Parameter`: cannot convert to traceable tracable  convert_tracable_parameter is set to False. - Check usage of context manager: do_not_convert_to_tracable_parameter - This graph break may be difficult to debug. Please report an issue to PyTorch for assistance."
  },
  {
    "id": "gb0229",
    "title": "GB0229",
    "url": "gb/gb0229.html",
    "content": "Unexpected type of data placeholder op for parameter construction data_node.op={data_node.op} Data node op should be placeholder or get_attr. - This graph break may be difficult to debug. Please report an issue to PyTorch for assistance."
  },
  {
    "id": "gb0230",
    "title": "GB0230",
    "url": "gb/gb0230.html",
    "content": "Attempted to use torch.use_deterministic_algorithms(warn_only=True) mode={mode}, warn_only={warn_only} Dynamo does not support this. - Remove param warn_only in function call torch.use_deterministic_algorithms. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0231",
    "title": "GB0231",
    "url": "gb/gb0231.html",
    "content": "call `torch.from_numpy` with `torch._dynamo.config.trace_numpy=False` trace_numpy={config.trace_numpy} Attempted to call `torch.from_numpy` with config `torch._dynamo.config.trace_numpy` set to `False`. - Change `torch._dynamo.config.trace_numpy` to `True`."
  },
  {
    "id": "gb0232",
    "title": "GB0232",
    "url": "gb/gb0232.html",
    "content": "`torch.from_numpy` with NumPy unavailable  Attempted to call `torch.numpy` but NumPy could not be imported. - Check NumPy version and installation in your environment. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0233",
    "title": "GB0233",
    "url": "gb/gb0233.html",
    "content": "Attempted to use strided NestedTensor layout={layout} Dynamo does not support this. - Change layout=torch.jagged. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0234",
    "title": "GB0234",
    "url": "gb/gb0234.html",
    "content": "Attempted to pop from empty torch function mode stack  Called `torch._C._pop_torch_function_stack` when torch function mode stack is empty. - Do not pop from empty torch function mode stack. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0235",
    "title": "GB0235",
    "url": "gb/gb0235.html",
    "content": "`torch.nn.Parameter` with non-constant Tensor attributes data={data} Dynamo does not support this. - Ensure the Tensor argument's shape, dtype, and device are correct. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0236",
    "title": "GB0236",
    "url": "gb/gb0236.html",
    "content": "Invalid input type for nonstrict_trace-ed function Encountered input of type <{type_name}>. For `nonstrict_trace`-ed functions, only basic types (e.g., torch.Tensor, int, float) or pytree containers of those are allowed as inputs. The provided argument contains an unsupported type. - Use one of the following to register the type with pytree:  - * `torch.utils._pytree.register_constant`  - * `torch.utils._pytree.register_dataclass`  - * `torch.utils._pytree.register_pytree_node`"
  },
  {
    "id": "gb0237",
    "title": "GB0237",
    "url": "gb/gb0237.html",
    "content": "non-constant `requires_grad` argument to `torch.nn.Parameter` requires_grad={requires_grad} Dynamo does not support this. - Change `requires_grad` to be a bool. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0238",
    "title": "GB0238",
    "url": "gb/gb0238.html",
    "content": "Input marked with `pytree.register_constant` constructed in the `torch.compile` region Input={input_spec_vt}, offending type <{type_name}>. Calling a `nonstrict_trace`-ed function with an input that contains an object of type <{type_name}>, which was marked with `pytree.register_constant`. However, the object was constructed _inside_ the `torch.compile` region. This is not supported. - Construct the object _outside_ the `torch.compile` region, or submit an issue to GitHub. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0239",
    "title": "GB0239",
    "url": "gb/gb0239.html",
    "content": "Invalid use of pytree_flatten with nonstrict_trace-ed function Input={input_spec_vt}, offending type <{type_name}>. Calling a `nonstrict_trace`-ed function where one of the inputs has been registered with a `pytree_flatten` that places an object of type <{type_name}> into the context. - Modifying the `pytree_flatten` to avoid placing the object into the context. - Apply one of the following to <{type_name}>:  - * `torch.utils._pytree.register_constant`  - * `torch.utils._pytree.register_dataclass`  - * `torch.utils._pytree.register_pytree_node` - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0240",
    "title": "GB0240",
    "url": "gb/gb0240.html",
    "content": "Shape mismatch with out= list of tensor variants fn={self.value}, args={args}, kwargs={kwargs} Shape mismatch when calling {self.value} with `out=`. Provided `out=` shape: {saved_out_shape}. Actual shape: {fake_out.shape}. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0241",
    "title": "GB0241",
    "url": "gb/gb0241.html",
    "content": "Attempted to call op with non-contiguous `out=` list of tensors self.value={self.value}, args={args}, kwargs={kwargs} Dynamo does not support this. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0242",
    "title": "GB0242",
    "url": "gb/gb0242.html",
    "content": "Attempted to call op with non-contiguous `out=` tensor self.value={self.value}, args={args}, kwargs={kwargs} Dynamo does not support this. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0243",
    "title": "GB0243",
    "url": "gb/gb0243.html",
    "content": "Attempted to use `torch.nn.modules.utils._ntuple` with unsupported argument type value={value} Dynamo does not support this. - Change use of _ntuple with argument as constant or tensor."
  },
  {
    "id": "gb0244",
    "title": "GB0244",
    "url": "gb/gb0244.html",
    "content": "Attempted to use `torch.nn.Parameter()` with export  Dynamo does not support this. - Do not use `torch.nn.Parameter()` with export. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0245",
    "title": "GB0245",
    "url": "gb/gb0245.html",
    "content": "Attempted to use `nested_tensor` with non-list input tensor_list={tensor_list} Dynamo does not support this. - Change `nested_tensor` with list input. - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0246",
    "title": "GB0246",
    "url": "gb/gb0246.html",
    "content": "Attempted to use `torch.nn.functional.one_hot` with data-dependent output shape args={args}, kwargs={kwargs} Dynamo does not support this. - Explicitly set the `num_classes` param of the function call  - `torch.nn.functional.one_hot` to something other than -1."
  },
  {
    "id": "gb0247",
    "title": "GB0247",
    "url": "gb/gb0247.html",
    "content": "Shape mismatch with out= tensor variant fn={self.value}, args={args}, kwargs={kwargs} Shape mismatch when calling {self.value} with `out=`. Provided `out=` shape: {saved_out_shapes}. Actual shape: {fake_out.shape}. - It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues."
  },
  {
    "id": "gb0248",
    "title": "GB0248",
    "url": "gb/gb0248.html",
    "content": "improper torch.get_device_module arguments args={args}, kwargs={kwargs} torch.get_device_module accepts 1 optional argument `device` - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0249",
    "title": "GB0249",
    "url": "gb/gb0249.html",
    "content": "bad device argument to torch.get_device_module args={args}, kwargs={kwargs} Expected valid string/torch.device argument ('cpu', 'cuda', etc.) - Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled."
  },
  {
    "id": "gb0250",
    "title": "GB0250",
    "url": "gb/gb0250.html",
    "content": "ndarray.astype(object) call_method {self} {name} {args} {kwargs} `ndarray.astype('O')` or `ndarray.astype(object)` is not supported by torch.compile, as there is no equivalent to object type in torch.Tensor. This will be executed eagerly. - This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround."
  }
]