---
layout: default
---
# GB0372

## Graph-Break Type
*Short name describing what triggered the graph break*

autograd.grad with external grad_fn

## Context
*Values or code snippet captured at the break point*

context

## Explanation
*Explanation of why the graph break was triggered*

torch.autograd.grad() cannot trace through the autograd graph because it's output depends on a tensor that was created outside the compiled region and has a grad_fn attached. The autograd graph extends beyond the compiled region boundary, which Dynamo cannot trace.

## Hints
*Hints on how to resolve the graph break*

- If you don't need gradients to flow back to the original tensor outside 
- the compiled region, detach the input: `tensor.detach().requires_grad_(True)`.
- Otherwise, move the autograd.grad() call outside the compiled region.
- It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues.


## Additional Information

<!-- ADDITIONAL INFORMATION START - Add custom information below this line -->

<!-- ADDITIONAL INFORMATION END -->


[Click here to add Additional Info](https://github.com/meta-pytorch/compile-graph-break-site/edit/main/docs/gb/gb0372.md)

[Back to Registry](../index.html)
