---
layout: default
---
# GB0371

## Graph-Break Type
*Short name describing what triggered the graph break*

autograd.grad with output that requires grad

## Context
*Values or code snippet captured at the break point*

context

## Explanation
*Explanation of why the graph break was triggered*

The compiled function uses torch.autograd.grad() and returns a tensor that still requires gradients and is connected to the autograd.grad() computation. This would cause aot_autograd to attempt 'backward through graph a second time', which is not supported.

## Hints
*Hints on how to resolve the graph break*

- Call .detach() on the output tensor before returning it from the compiled function.
- For example: `return loss.detach(), grads` instead of `return loss, grads`.
- Or set create_graph=False in autograd.grad() if you don't need higher-order gradients.
- Your code may result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled. You can do this by removing the `torch.compile` call, or by using `torch.compiler.set_stance("force_eager")`. 


## Additional Information

<!-- ADDITIONAL INFORMATION START - Add custom information below this line -->

<!-- ADDITIONAL INFORMATION END -->


[Click here to add Additional Info](https://github.com/meta-pytorch/compile-graph-break-site/edit/main/docs/gb/gb0371.md)

[Back to Registry](../index.html)
