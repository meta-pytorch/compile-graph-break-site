---
layout: default
---
# GB4927

## Graph-Break Type
*Short name describing what triggered the graph break*

autograd.grad consumed returned tensor's grad_fn

## Context
*Values or code snippet captured at the break point*



## Explanation
*Explanation of why the graph break was triggered*

torch.autograd.grad() consumes grad_fns that are needed by tensors returned from this compiled function. This would cause 'backward through graph a second time' errors.

## Hints
*Hints on how to resolve the graph break*

- If you don't need to backward through the returned tensor, 
- call .detach() before returning: `return loss.detach()`
- If you need to backward through the returned tensor, use retain_graph=True in autograd.grad().


## Additional Information

<!-- ADDITIONAL INFORMATION START - Add custom information below this line -->

<!-- ADDITIONAL INFORMATION END -->


[Click here to add Additional Info](https://github.com/meta-pytorch/compile-graph-break-site/edit/main/docs/gb/gb4927.md)

[Back to Registry](../index.html)
