---
layout: default
---
# GB1381

## Graph-Break Type
*Short name describing what triggered the graph break*

autograd.grad with already consumed grad_fn

## Context
*Values or code snippet captured at the break point*

double consumed grad_fns: {len(double_consumed)}

## Explanation
*Explanation of why the graph break was triggered*

torch.autograd.grad() is trying to consume grad_fns that were already consumed by a previous autograd.grad() call. This would cause 'backward through graph a second time' errors at runtime.

## Hints
*Hints on how to resolve the graph break*

- Use retain_graph=True in the first autograd.grad() call if you 
- need to compute gradients through the same graph multiple times.


## Additional Information

<!-- ADDITIONAL INFORMATION START - Add custom information below this line -->

<!-- ADDITIONAL INFORMATION END -->


[Click here to add Additional Info](https://github.com/meta-pytorch/compile-graph-break-site/edit/main/docs/gb/gb1381.md)

[Back to Registry](../index.html)
